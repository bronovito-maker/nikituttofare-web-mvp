Codex Implementation Notes
==========================

Context
-------
Periodo: evoluzione flusso prenotazioni dopo pairing con Codex (GPT-5).
Obiettivo: passare da un percorso a step rigidi a un assistente proattivo che riempie slot in modo flessibile, comprende frasi colloquiali e sfrutta lo storico cliente per accelerare la prenotazione.

Interventi principali
---------------------

1. Prompt di sistema evoluto (`lib/prompt-builder.ts`)
   - Nuova API `buildSystemPrompt(tenantId, { customerProfile })` che accetta un profilo cliente opzionale.
   - Sezione “Prenotazioni — Slot Flessibili” aggiornata: lo script spiega all’LLM come analizzare ogni messaggio, evitare salti di slot, chiedere chiarimenti su informazioni vaghe (“stasera”, “verso le 21”) e gestire modifiche successive con conferma esplicita.
   - Nuova sezione “Dati Cliente Autenticato” (popolata quando disponibile) con nome riconosciuto, telefono archiviato, numero prenotazioni totali, riepilogo ultima prenotazione, party size preferito e orari abituali. Il prompt istruisce il modello a usare questi insight per proposte proattive ma senza saltare la conferma degli slot.

2. Parser e NLU potenziati (`lib/chat-parser.ts`, `lib/customer-personalization.ts`, `app/api/nlu/route.ts`)
   - `parseChatData` diventa asincrona: esegue prima l’analisi regex storica e poi chiama l’endpoint NLU `/api/nlu` per un estrattore basato su LLM (`gpt-4o-mini`) che restituisce JSON strutturato (intent, slot, note). In caso di errore/timeout torna al solo parsing euristico.
   - Introduzione di pattern per individuare frasi ambigue su date/orari (es. “nel weekend”, “dopo le nove”); il parser allega una lista `clarifications` con slot da confermare e testo incriminato.
   - Nuova normalizzazione comune (telefono, ISO datetime, note) e fusione dei dati LLM + euristici con pulizia delle note e deduplicazione party size.
   - Aggiunto `lib/customer-personalization.ts`: recupera profilo cliente partendo dall’email della sessione, legge customer + ultime prenotazioni, calcola party size preferito e orari più frequenti per alimentare il prompt.
   - Endpoint `/api/nlu` (runtime node) che chiama OpenAI con system prompt dedicato, forza `response_format: json_object` e gestisce validazioni/timeout. Autenticazione basata su `auth()` per garantire accesso solo al tenant loggato.

3. Hook `useChat` orientato agli slot flessibili (`hooks/useChat.tsx`)
   - Stato dei campi riscritto in `BookingSlots`: per ogni slot manteniamo valore, provenienza (`parser`/`manual`), timestamp e flag `needsClarification` con relativo motivo. Ogni parse aggiorna anche la lista `clarifications` e i `recentlyUpdatedSlots`.
   - `bookingData` ora espone solo slot completi e non ambigui; i campi in attesa di chiarimento non vengono usati per il riepilogo o per l’invio alle API.
   - `missingSteps` include anche slot che richiedono chiarimento, così l’interfaccia continua a mostrarli come incompleti.
   - Nuovo auto-trigger di conferma: quando il riepilogo è pronto e l’utente scrive frasi positive (“confermo”, “sì va bene”, “ok procedi”), l’hook avvia `handleConfirmBooking()` evitando click manuale. Un set locale impedisce doppie conferme sullo stesso messaggio e filtra frasi dubitative/negative.
   - Pulizia dello stato (slot, clarificazioni, conferme) al reset della chat e serializzazione sicura dei messaggi per le API `/api/leads` e `/api/bookings`.

4. Interfaccia chat dinamica (`components/chat/ChatInterface.tsx`, `components/chat/MessageInput.tsx`)
   - Progress bar sostituita da “Panoramica prenotazione”: card per slot in griglia con stato (mancante / da confermare / completo), valore corrente, tooltip di chiarimento e highlight temporaneo per gli slot aggiornati dall’AI.
   - Click su una card precompila l’input utente con un template contestuale (es. “Aggiorno l’orario a …”) e porta il focus sul campo per velocizzare le correzioni. `MessageInput` ora riceve un ref esterno per gestire il focus.
   - Il template inserisce un placeholder `"..."` selezionato automaticamente: l’utente può digitare subito il nuovo valore senza cancellare manualmente il testo suggerito.
   - Il riepilogo rimane nel feed ma appare solo quando tutti gli slot sono confermati, mentre sotto all’input compare un promemoria sintetico con gli slot mancanti/da confermare.

5. Integrazione personalizzata lato backend (`app/api/assist/route.ts`)
   - L’handler, oltre a costruire il prompt, tenta di caricare il profilo cliente tramite l’email presente in sessione. In caso di successo, passa l’oggetto a `buildSystemPrompt`; in caso di errore logga un warning e prosegue senza interrompere il flusso.
   - Stream verso OpenAI invariato, ma ora alimentato da prompt arricchito con insight storici.
   - Il recupero del profilo cliente usa una cache in memoria con TTL 5 minuti (`lib/customer-personalization.ts`), riducendo le chiamate ripetute a NocoDB durante la stessa conversazione.

6. Endpoint e stato per conferma multimodale
   - `useChat` marca automaticamente i messaggi di conferma già processati e gestisce conflitti con slot ambigui.
   - Il pulsante “Conferma prenotazione” rimane disponibile, ma la conferma testuale viene intercettata solo quando il messaggio è totalmente affermativo: la logica esclude frasi con domande o richieste di modifica e invia feedback d’errore in caso di mancanza dati o fail lato NocoDB.
   - Gli aggiornamenti degli slot provenienti dal parser sono ora ordinati tramite timestamp: un parse lento non può sovrascrivere correzioni manuali o dati più recenti (`hooks/useChat.tsx`).

7. Affidabilità NLU e gestione fallback (`lib/chat-parser.ts`, `app/api/nlu/route.ts`)
   - Il parser euristico ora restituisce anche una mappa di confidenza per ogni slot (telefono, data/ora, party size). La fase di merge dà priorità all’NLU salvo quando l’euristica ha match “high confidence” (es. telefono completo, ISO datetime) e l’NLU fornisce valori parziali o invalidi.
   - L’NLU produce, per ogni slot, un flag `*_is_ambiguous` che alimenta direttamente il `needsClarification`; le regex di ambiguità entrano in gioco solo se l’endpoint fallisce.
   - Logging migliorato: quando l’NLU fallisce si emette un warning mirato prima di ricadere sulle regex.
   - L’endpoint `/api/nlu` misura la durata delle chiamate OpenAI, logga ogni errore (inclusi timeout) e applica un singolo retry con exponential backoff breve per status retryable (408/429/5xx) o timeout.

8. Test automatici e tooling (`vitest.config.ts`, `lib/chat-parser.test.ts`, `hooks/useChat.test.tsx`)
   - Configurato Vitest con ambiente JSDOM, alias `@` e setup condiviso (`vitest.config.ts`, `test/setup.ts`) includendo Testing Library e Jest DOM.
   - Suite `lib/chat-parser.test.ts`: mock dell’endpoint `/api/nlu` per validare fallback, priorità NLU/regex e propagazione delle ambiguità.
   - Suite `hooks/useChat.test.tsx`: test della state machine (slot in ordine sparso, blocco/sblocco `summaryReady`, auto-conferma affidabile, conflitti parser vs manual) con fetch e `parseChatData` stubbati.
   - Nuovo script `npm run test` documentato in `package.json`.

Comportamento finale atteso
---------------------------
1. L’assistente compila gli slot in qualsiasi ordine, riconosce frasi colloquiali, segnala chiarimenti quando l’utente è vago e conferma solo dopo aver raccolto dati espliciti.
2. L’interfaccia mostra in tempo reale lo stato di ogni slot, evidenziando quelli aggiornati dall’AI e permettendo correzioni rapide con un click.
3. Il riepilogo finale riflette esclusivamente dati confermati (nessun valore ambiguo o incompleto) e la conferma può avvenire sia via pulsante sia via messaggio testuale.
4. Utenti autenticati vengono accolti in maniera proattiva: nome precompilato, suggerimenti su party size/orario abituali e reminder dell’ultima prenotazione.
5. Tutte le richieste a NocoDB restano dietro conferma esplicita dell’utente, garantendo coerenza tra lead e booking.

Suggerimenti futuri
-------------------
- Estendere il recupero storico anche a preferenze specifiche (tavolo finestra, allergie ricorrenti) e collegarlo a prompt UI (“Vuoi lo stesso tavolo?”).
- Aggiungere test automatici sul parser (mockando l’endpoint `/api/nlu`) e sulla logica di auto-conferma per evitare regressioni.
- Valutare un caching leggero della personalizzazione per ridurre le chiamate a NocoDB su conversazioni lunghe.
- Documentare nel README la nuova pipeline slot → NLU → slot state e le istruzioni per configurare i modelli OpenAI dedicati (chat vs parser).
